{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a38eb7ae-3f15-492a-8054-328f803c0a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import imghdr\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d440c30-f54c-42de-b9bb-32b4b2d21f48",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Reading, cleaning and processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb83c8a-a234-45c3-971a-931bf56093fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Clean the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9e8ef7-f62c-48da-8650-fe74ada14b9f",
   "metadata": {},
   "source": [
    "Define the directory of the data and remove any images with different extensions than the ones specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b912fc91-7cac-48d1-8f48-cfd393600708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'\n",
    "IMG_EXT = ['png', 'jpg', 'jpeg', 'bmp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "990ed909-7446-4c54-90e2-aec5cb8ce2ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_images(data_dir : str, extensions : list):\n",
    "    '''\n",
    "        Looks for subfolders (which should be the classes to classify)\n",
    "        of data_dir and removes the images which have \n",
    "        other extensions than the ones specified.\n",
    "    '''\n",
    "    for img_class in os.listdir(data_dir):                            # List every img class in data_dir\n",
    "        for image in os.listdir(os.path.join(data_dir, img_class)):   # For every class, list the images\n",
    "            image_path = os.path.join(data_dir, img_class, image)     # Get the path of the image\n",
    "            try:\n",
    "                img = cv2.imread(image_path)                          # Read the img with opencv - unnecessary?\n",
    "                tip = imghdr.what(image_path)                         # Get the extension of the image\n",
    "                if tip not in extensions:                             \n",
    "                    os.remove(image_path)                             # Remove img if it has an unwanted extension\n",
    "            except Exception as e:\n",
    "                print(f'Issue with {image_path}')                     # Print a msg if there's an error with any img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84f430fb-b7f2-48da-8a7a-ee1e5a52cd82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_images(DATA_DIR, IMG_EXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9170f7fe-243c-441a-b321-87811afb90a7",
   "metadata": {},
   "source": [
    "### 2. Read and transform the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fcdf10-3049-45b5-b03e-c66b67194785",
   "metadata": {},
   "source": [
    "The images must be resized to have the same resolution and must be a PyTorch tensor.\n",
    "\n",
    "PyTorch has a really handy way of implementing these transformations through `torchvision.transforms`.\n",
    "\n",
    "* `torchvision.transforms.Compose` creates a list of transformations that will be applied sequentially.\n",
    "* `torchvision.transforms.Resize((255, 255))` resizes the data to the specified size.\n",
    "* `torchvision.transforms.ToTensor()` converts the data into a torch.Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "88b96a7d-0248-43e7-8b3a-3b382a478e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.Resize((255, 255)),\n",
    "                                            torchvision.transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09012132-b881-4fcf-8518-e1b1c482d716",
   "metadata": {},
   "source": [
    "Now through `torchvision.datasets.ImageFolder` read the directory of the data.\n",
    "\n",
    "Note that the directory data must have subfolders like 'meme', 'family', 'dinner' that will be treated as\n",
    "the classes for image classification.\n",
    "\n",
    "Through the same method, the transformations can be applied by providing a value to the `transform` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b6d31291-7cfa-4757-8768-7aeb9b66078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torchvision.datasets.ImageFolder(DATA_DIR, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03982efa-56c6-482a-a8c5-0142283faf90",
   "metadata": {},
   "source": [
    "The classes and their corresponding label can be seen with `class_to_idx`. Remember that this is not an array or Tensor, just a container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d01ace44-8afd-4aa2-a004-63641ca3016e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'family': 0, 'memes': 1}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.class_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9e839d-b340-44c4-ad93-a4085d2d3f7e",
   "metadata": {},
   "source": [
    "The image samples can be accessed through `data.samples` and their respective labels with `data.targets`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2828d71-be53-4f48-9407-fbe4b9e08767",
   "metadata": {},
   "source": [
    "### 3. Create the data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4075c129-8f87-4198-bef1-73d466c871b3",
   "metadata": {},
   "source": [
    "For better training, it is useful to separate the data in  multiple batches and shuffle the samples so that each batch has images from both classes.\n",
    "\n",
    "A `DataLoader` in PyTorch isn't a list or a dataset that contains all the batches. Instead, it loads eatch sample (or batch) one at a time into memory. Basically, it can be treated as a generator.\n",
    "\n",
    "Creating a `DataLoader` is easy. Use `torch.utils.data.DataLoader` and specify the dataset (in this case is the images loaded in `data`). At the same time, specify `batch_size` and `shuffle` so that PyTorch may automatically split the data and randomly shuffle it.\n",
    "\n",
    "To get the next entry manually from `dataloader` one must call `next(iter(dataloader))`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9d060db7-db47-4287-9cfb-168986b22715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset=data,   # The dataset\n",
    "                                         batch_size=23,  # Each batch will have 23 elements\n",
    "                                         shuffle=True)   # Shuffle the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821bca7-0bf4-4cc0-93e0-daa0cd134f81",
   "metadata": {},
   "source": [
    "`batch_size` is 23 because there are 230 images for each class, therefore 460 samples in total. Just so that every batch has the same number of entries, I decided to use a number so that `batch_size % number` is 0. So, there will be 20 batches (460 / 23)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f1b35008-8758-485b-8581-559179e28ff8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader) # Number of batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4416c96a-a27c-404f-89ec-d96debab0121",
   "metadata": {},
   "source": [
    "### 4. Split the data into train, test and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e11b8a-bfe9-45fc-aa25-f5e7145cb6c4",
   "metadata": {},
   "source": [
    "Select a manual seed just so that the results can be reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3541ec9a-7c17-4e3b-b4db-1f40f72fd80d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen1 = torch.Generator().manual_seed(69) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e69ce-2dc4-4622-87ca-061b0be3361d",
   "metadata": {},
   "source": [
    "Really easy to split the data when using a `DataLoader`. Just use `torch.utils.data.random_split`, specify a loader, the percentage of the original data that each different set will represent and optionally a generator.\n",
    "\n",
    "Note that `[0.7, 0.15, 0.15]` sums up to 1, then the first set will be 70% of `dataloader`, and the other two sets will split the remaining 30% of `dataloader` between them, so 15% each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1006ca7b-a1e2-443f-9328-e4abb11a32ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set, test_set, val_set = torch.utils.data.random_split(dataloader, [0.7, 0.15, 0.15], generator=gen1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d16ddc3-8b9c-470e-ad24-7459fb26e0cc",
   "metadata": {},
   "source": [
    "Remember there are 20 batches in this case. The sum of each set's length must add to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "48edcd7f-9a4e-47e9-a927-73cfbb13965c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "print(len(test_set))\n",
    "print(len(val_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21a5a62-cba4-444f-8087-b2cdaad66e24",
   "metadata": {},
   "source": [
    "### 5. IMPORTANT NOTE ABOUT THE GENERATED SETS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321c5f25-7d4b-4f0a-9298-1755215f54ad",
   "metadata": {},
   "source": [
    "Each set contains *BOTH* the data and the labels. So `train_set` contains the X_train and the y_train in *BATCHES*, the same applies for the test_set and val_set.\n",
    "\n",
    "To access actual data, one must iterate through `set.dataset`. This is because `set.dataset` returns a DataLoader generator, but `set` has a type of Subset, which is not iterable nor indexable.\n",
    "\n",
    "Also, when iterating `set.dataset` each element that it returns is a tuple in which the first element is the data and the second one are the labels for each sample in data for that batch. Just unwrap them like `x_batch, y_batch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cfb91721-4274-4c2a-9ffc-efff2fed3c0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataset.Subset"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "341329c4-6ce2-43c0-bac3-258f320fafbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_set.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25df9022-02c5-4049-822b-0391ef3d13f1",
   "metadata": {},
   "source": [
    "# The Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f1497b-8bf3-4f6f-af18-b8f5192d1d12",
   "metadata": {},
   "source": [
    "### 1. The architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c30cff-4c34-42fb-8fbd-01d5394bad19",
   "metadata": {},
   "source": [
    "The model will consist of 3 convolutional blocks, 1 flatten layer and 2 fully connected linear layers.\n",
    "\n",
    "In turn, each convolutional block is a sequential model containing:\n",
    "* The convolutional layer.\n",
    "* The activation layer, ReLU in this case.\n",
    "* The pooling layer, MaxPooling2D in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd804b48-dbc6-49e0-9397-71bb1de4667a",
   "metadata": {},
   "source": [
    "#### 1.1. The convolution blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d7612b-7387-4434-b60a-b88aaeac5acc",
   "metadata": {},
   "source": [
    "*REMEMBER*: Each convolutional layer has a kernel of size 3x3. This means that each time the data goes through these layers it will come out with 2 pixel less because the kernel won't fit completely at the edges of the image. Then, 1 pixel is ignored on the left, right, the top and bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa90828c-8298-4a7a-bf4b-d7b998ff6a5f",
   "metadata": {},
   "source": [
    "##### 1.1.1 First convolutional block\n",
    "-> input with shape `[23, 3, 255, 255]`.\n",
    "\n",
    "Remember that the data will come with a size of `[23, 3, 255, 255]` since each batch has 23 elements, each image has 3 color channels (RGB) and a dimension of 256x256 (it shows 255 because the 0th element). Because of the 3 channels, the convolution layer will have 3 has the input channels.\n",
    "\n",
    "After the convolution and activation, the data has a shape of `[23, 16, 253, 253]`, so each image has a res of 254x254 now.\n",
    "\n",
    "The MaxPooling2D will halve the resolution to 127x127.\n",
    "\n",
    "-> data output with shape of `[23, 16, 126, 126]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aade860-0d3e-431b-9504-87ff375dadb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 1.1.2 Second convolutional block\n",
    "-> input shape `[23, 16, 126, 126]`.\n",
    "\n",
    "It has 16 channels as input since the first convolutional block has an output of 16 channels. It will output 32 channels.\n",
    "\n",
    "After convolution & activation -> data with shape `[23, 32, 124, 124]`. Each img has res of 125x125.\n",
    "\n",
    "For the pooling, halving the shape will result in `[23, 32, 62, 62]`.\n",
    "\n",
    "-> output with shape `[23, 32, 62, 62]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9fa14a-772c-487a-a96e-cfd187505973",
   "metadata": {},
   "source": [
    "##### 1.1.3 Third convolutional block\n",
    "-> input shape `[23, 32, 62, 62]`.\n",
    "\n",
    "32 channels as input because seconds convolutional block outputs 32 channles. It will output 16 channels.\n",
    "\n",
    "After convolution & activation -> data with shape of `[23, 32, 60, 60]`.\n",
    "\n",
    "After pooling -> data with shape of `[23, 32, 30, 30]`\n",
    "\n",
    "-> output with shape `[23, 16, 30, 30]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604db720-5659-4d4e-b6b6-57c34c4ed4d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.2 The flatten layer\n",
    "-> input with shape [23, 16, 30, 30].\n",
    "\n",
    "When using `nn.Flatten(start_dim=1, end_dim=-1)` we are telling PyTorch to flatten the input array leaving the first dimension (index 0) the same but flattening all dimensions remaining. Remember that in Python the -1 means last element, in this case is the last dimension.\n",
    "\n",
    "So, to know the shape of the output just multiply the length of each flattened dimension. In this case it would be 16 * 30 * 30 = 14400.\n",
    "\n",
    "-> output with shape `[23, 14400]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bcf53b-0aa2-4ad6-909d-a4e61bb97408",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.3 The dense blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60186ac4-3b8b-4992-b7bf-7c67c9f5b831",
   "metadata": {},
   "source": [
    "##### 1.3.1 First dense block\n",
    "-> input shape `[23, 14400]`.\n",
    "\n",
    "The shape of `nn.Linear(23, 16 * 30 * 30)` is explained in the flatten layer.\n",
    "Just apply the ReLU to the output of the linear layer.\n",
    "\n",
    "-> output shape `[23, 10]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a09adbd-029d-43de-bf21-07210d888b26",
   "metadata": {},
   "source": [
    "##### 1.3.2 Second dense block\n",
    "-> input shape `[23, 10]`\n",
    "\n",
    "Input of 10 because the last dense block outputs 10 values.\n",
    "Just apply the sigmoid activation function to the outputs of the linear layer.\n",
    "\n",
    "-> output shape `[23, 1]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29bc525-a277-480e-8406-ad2b8106c273",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.4 The output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52314575-ceba-4ff0-8020-ca4659ce1e11",
   "metadata": {},
   "source": [
    "Since the output has an extra dimension with a length of 1, just squeeze the tensor to remove this dimension.\n",
    "\n",
    "This way, the output is just a 1D tensor that has the predicted class for each entry in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0d916b8d-497f-4b41-b3cb-a8b69f64c684",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1_block = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=(3, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.conv2_block = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=(3, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.conv3_block = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, kernel_size=(3, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "        \n",
    "        self.dense_block = nn.Sequential(\n",
    "            nn.Linear(16 * 30 * 30, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(10, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x : torch.Tensor):\n",
    "        x = self.conv1_block(x)\n",
    "        x = self.conv2_block(x)\n",
    "        x = self.conv3_block(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_block(x)\n",
    "        return self.output(x).squeeze()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d3337-908b-4410-a8df-c101b01d536a",
   "metadata": {},
   "source": [
    "Instanciate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8726b8a6-84c0-4607-a02a-1ce27fb07ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e746f6d8-6957-4309-a3ff-ce0e2920579a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " # Optimizer and loss\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b8665e20-f613-4c9c-8b88-4b98104d26db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (conv1_block): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2_block): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv3_block): Sequential(\n",
      "    (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (dense_block): Sequential(\n",
      "    (0): Linear(in_features=14400, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (output): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "988be0f1-359e-4e40-ae48-99d54ab3a3e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "366e5d99-8fca-41d4-becc-19aba0379104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_nn(epochs, net : torch.nn.Module, \n",
    "             train_set : torch.utils.data.DataLoader,\n",
    "             val_set : torch.utils.data.DataLoader) -> (torch.nn.Module, list, list):\n",
    "    \n",
    "    net.train()                                              # Set the NN to training mode.\n",
    "    \n",
    "    epoch_count, train_loss_vals, val_loss_vals = [], [], [] # List that will be used to check model performance\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        for x_batch, y_batch in train_set.dataset:           # Joint the corresponding batches and unwrap them\n",
    "            y_batch = y_batch.type(torch.float32)\n",
    "            y_pred = net(x_batch)                            # Make predictions for the current batch\n",
    "            loss = loss_fn(y_pred, y_batch)                  # Compute the loss of the predictions\n",
    "            optimizer.zero_grad()                            # Reset the gradients\n",
    "            loss.backward()                                  # Perform the backpropagation\n",
    "            optimizer.step()                                 # Optimize the model\n",
    "            \n",
    "        for data, labels in val_set.dataset:\n",
    "            labels = labels.type(torch.float32)\n",
    "            preds = net(data)\n",
    "            val_loss = loss_fn(preds, labels)\n",
    "    \n",
    "        # if epoch % 10 == 0:\n",
    "        epoch_count.append(epoch)                            \n",
    "        train_loss_vals.append(loss)\n",
    "        val_loss_vals.append(val_loss)\n",
    "        print(f\"Epoch {epoch}: Train loss of {loss}; Validation loss of {val_loss}\")\n",
    "        # print(net.state_dict())                            # Bad idea, the state_dict is huge\n",
    "            \n",
    "    return net, epoch_count, train_loss_vals, val_loss_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6096552a-0708-483c-baf9-f63f2d07bd89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss of 0.6360604763031006; Validation loss of 0.6450244784355164\n"
     ]
    }
   ],
   "source": [
    "trained_model, epoch_l, train_loss_l, val_loss_l = train_nn(40, model, train_set, val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19b95d43-db1c-48ff-b1f4-1297edc426f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0f5abd9-f34d-4a24-87c2-af0a6016987a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model(model : nn.Module, file_name : str) -> None:\n",
    "    MODEL_DIR = Path('MODELS')\n",
    "    MODEL_NAME = f'{file_name}.pth'\n",
    "    MODEL_PATH = MODEL_DIR / MODEL_NAME\n",
    "    \n",
    "    torch.save(obj=trained_model.state_dict(),\n",
    "               f=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d87557-e5a4-4405-bb97-a4bedabd3e95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_model(trained_model, 'trained_model_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f0a3b9-e005-4433-8b12-9e9f7371d38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "3d927f6f-417f-460a-a570-a4de9b64c85d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8e8f398f-bbf2-4509-ac56-f6e17a0eb239",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch_l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m ax2 \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m----> 4\u001b[0m     ax1\u001b[38;5;241m.\u001b[39mplot(\u001b[43mepoch_l\u001b[49m, train_loss_l)\n\u001b[0;32m      5\u001b[0m     ax2\u001b[38;5;241m.\u001b[39mplot(epoch_l, val_loss_l)   \u001b[38;5;66;03m# Empty: forgot to add the append inside the train loop\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'epoch_l' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax1 = plt.subplot(2, 2, 1)\n",
    "ax2 = plt.subplot(2, 2, 2)\n",
    "with torch.inference_mode():\n",
    "    ax1.plot(epoch_l, train_loss_l)\n",
    "    ax2.plot(epoch_l, val_loss_l)   # Empty: forgot to add the append inside the train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd23df48-3d72-4cea-a879-9afc86136cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_model(model : nn.Module):\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for test_data, test_label in test_set.dataset:\n",
    "            test_pred = model(test_data)\n",
    "            test_loss = loss_fn(test_pred, test_label.type(torch.float32))\n",
    "            print(test_loss)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a9c0859-75f4-4ee3-9724-b933dd257b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "79a96aef-2d32-49a5-91a1-5970faa70803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_state_d(file_name : str) -> Model:\n",
    "    MODEL_DIR = Path('MODELS')\n",
    "    MODEL_NAME = f'{file_name}.pth'\n",
    "    MODEL_PATH = MODEL_DIR / MODEL_NAME\n",
    "    \n",
    "    state_d = torch.load(MODEL_PATH)\n",
    "    m = Model()\n",
    "    m.load_state_dict(state_d)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dfd20db4-3878-4e1b-b6fd-6f3340f43518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded_model = load_state_d('trained_model_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ec2e3159-85d2-41ad-a50d-4fa3189e59ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loaded_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "399e6ea1-0da9-413a-9250-d5435010d542",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0001)\n",
      "tensor(9.4315e-05)\n",
      "tensor(5.8615e-05)\n",
      "tensor(5.1841e-05)\n",
      "tensor(0.0001)\n",
      "tensor(5.9789e-05)\n",
      "tensor(6.2462e-05)\n",
      "tensor(8.5722e-05)\n",
      "tensor(2.2999e-05)\n",
      "tensor(5.0157e-05)\n",
      "tensor(3.0289e-05)\n",
      "tensor(4.7441e-05)\n",
      "tensor(3.2242e-05)\n",
      "tensor(0.0001)\n",
      "tensor(3.2469e-05)\n",
      "tensor(0.0001)\n",
      "tensor(0.0001)\n",
      "tensor(0.0001)\n",
      "tensor(0.0002)\n",
      "tensor(0.0002)\n"
     ]
    }
   ],
   "source": [
    "test_model(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a7bda380-7a9e-47f5-a6f0-9dc32ddc6f73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test on a real image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0cdfce28-3176-4a30-ba45-97b5757ca24d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEST_IMGS = Path('TEST_IMGS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "15c16315-5674-4880-bf6a-f23e79b796d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = Image.open(TEST_IMGS / 'nanny_test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "52439059-32bb-4519-ac23-253a18a78955",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nanny_test = transform(img)\n",
    "nanny_test = nanny_test[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "07c41a4d-ca69-4df2-a256-02978ee076f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = loaded_model(nanny_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0f08f678-300e-4f60-8804-88f793a0462b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0006], grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "983cd064-1089-467f-b2c6-6216bc207a64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# On a meme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b674f2e0-285b-4696-899f-2189fafc8263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_meme = Image.open(TEST_IMGS / 'meme_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e42f73bd-047d-4b42-9462-a02e91ad081d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meme_test = transform(img_meme)\n",
    "meme_test = meme_test[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "074ddc60-afbb-4c56-a040-68950cf7e344",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred2 = loaded_model(meme_test[:, :3, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "61e75ac9-37eb-454a-8a6a-75302dbdb9c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.], grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
